** Classification supervisée pour les séries multi-temporelles       :slides:
*** Objectifs et Données
**** Objectifs
      Les objectifs sont les suivants:
     - Connaître les différentes applications constituant la procédure
       de classification supervisée
     - Utiliser différents algorithmes pour l'apprentissage
     - Savoir mesurer les performances de la classification
     - Connaître les post-traitements applicables à une classification
**** Données
     Les données sont disponibles dans le répertoire ~Data/classification~, avec les sous-répertoires suivants:
     - ~Extract16bits~ contient la série multi-temporelle LandSat8,
     - ~training~ contient la donnée d'apprentissage au format /shp/,
     - ~testing~ contient la donnée de validation au format /shp/.
     
*** Déroulement
    les étapes de l'exercice sont les suivantes:
    1. Introduction aux données landsat8
    2. Classification mono-date
    3. Classification multi-date
    4. Classification avec profil de NDVI
    5. Post-traitements de la classification
    
*** Présentation des données LandSat8

    *Résolution spatiale:* 30 mètres

**** Dates :BMCOL:B_block:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :BEAMER_env: block
     :END:

|------------|
| 2014-03-09 |
| 2014-04-01 |
| 2014-04-17 |
| 2014-05-28 |
| 2014-06-20 |
| 2014-07-31 |
| 2014-09-01 |
| 2014-10-03 |
| 2014-10-26 |
|------------|

**** Bandes :BMCOL:B_block:
     :PROPERTIES: 
     :BEAMER_col: 0.5
     :BEAMER_env: block
     :END:

 |---+---------------------|
 | 0 | Coastal aerosol     |
 | 1 | Blue                |
 | 2 | Green               |
 | 3 | Red                 |
 | 4 | Near Infrared (NIR) |
 | 5 | SWIR 1              |
 | 6 | SWIR 2              |
 |---+---------------------|

*** Présentation des données de référence

|------+-----------------------------+------------|
| Code | Nom                         | #polygones |
|------+-----------------------------+------------|
|   11 | Éte                         | 7898       |
|   12 | Hiver                       | 8171       |
|   31 | Foret feuilles caduques     | 867        |
|   32 | Foret feuilles persistantes | 125        |
|   34 | Pelouses                    | 45         |
|   36 | Lande ligneuse              | 386        |
|   41 | Bati                        | 4719       |
|   51 | Eau                         | 1280       |
|  211 | Prairie                     | 5647       |
|  221 | Verger                      | 204        |
|  222 | Vigne                       | 559        |
|------+-----------------------------+------------|

*** Classification supervisée
    #+ATTR_LATEX: :float t :width \textwidth
    [[file:Images/classification.png]]
   
*** Algorithme SVM
    #+ATTR_LATEX: :float t :width 0.5\textwidth
    [[file:Images/svm.png]]

*** Algorithme RF
    Ensemble d'arbres de décision aléatoires

**** Apprentissage
     1. Séparer le jeu d'apprentissage en k ensembles $S_k$ aléatoires
     2. Pour chaque $S_k$ choisir aléatoirement $F_k$ primitives
     3. Construire un arbre de décision récursivement, pour chaque noeud:
        1. Choisir $f \in F_k$ et le seuil $t_k$ qui sépare l'ensemble restant en 2 parties les plus pures
        2. Arrêter quand l'ensemble restant devient trop petit
 
**** Décision
     Vote majoritaire de tous les arbres aléatoires


*** Matrice de confusion


|-----------+--------------+--------------+--------------+
|           | Préd. 1      | Préd. 2      | Préd. 3      | 
|-----------+--------------+--------------+--------------+
| Réf. 1    | Vrais pos. 1 |              |              |
| Réf. 2    |              | Vrais pos. 2 |              |
| Réf. 3    |              |              | Vrais pos. 3 |
|-----------+--------------+--------------+--------------+

- $precision = \frac{VP i}{\sum pred. i}$
- $rappel = \frac{VP i}{T\sum ref. i}$
- $Accuracy = \frac{\sum{VP i}}{Total}$
- $Kappa = \frac{Accuracy - chance}{1-chance}$

  
** Classification supervisée pour les séries multi-temporelles        :guide:
*** Description                                                        :desc:
**** Résumé

     Cet exercice permet de se familiariser avec les applications de
     classification supervisée pixellique de l'Orfeo ToolBox, en
     utilisant une série multi-temporelle Landsat8 et un jeu de
     données de référence pour la supervision.

**** Pré-requis
     
     - Logiciels installés (Monteverdi et Orfeo ToolBox)
     - Données téléchargées
     - Connaissance du mécanisme des applications de l'Orfeo ToolBox (voir exercice correspondant)
     - Notions de classification supervisée
     
**** Objectifs

     Les objectifs sont les suivants:
     - Connaître les différentes applications constituant la procédure
       de classification supervisée
     - Utiliser différents algorithmes pour l'apprentissage
     - Savoir mesurer les performances de la classification
     - Connaître les post-traitements applicables à une classification

*** Étapes                                                            :steps:

    Les données sont disponibles dans le répertoire ~Data/classification~, avec les sous-répertoires suivants:
     - ~Extract16bits~ contient la série multi-temporelle LandSat8,
     - ~training~ contient la donnée d'apprentissage au format /shp/,
     - ~testing~ contient la donnée de validation au format /shp/.

**** Présentation des données LANDSAT 8

    Dans l'archive de données, le dossier ~Data/classification/Extract16bits~ contient neuf
    images Landsat 8 aux dates suivantes:
    
     |------------|
     | 2014-03-09 |
     | 2014-04-01 |
     | 2014-04-17 |
     | 2014-05-28 |
     | 2014-06-20 |
     | 2014-07-31 |
     | 2014-09-01 |
     | 2014-10-03 |
     | 2014-10-26 |
     |            |
     |------------|

    Ces images sont toutes multispectrales avec les sept bandes du capteur OLI:

    |---+---------------------|
    | 0 | Coastal aerosol     |
    | 1 | Blue                |
    | 2 | Green               |
    | 3 | Red                 |
    | 4 | Near Infrared (NIR) |
    | 5 | SWIR 1              |
    | 6 | SWIR 2              |
    |---+---------------------|

    Au total, c'est donc 63 bandes qui représentent chaque pixel.
    Les images sont encodés sur 16 bits.

    Ouvrez une image dans monteverdi et régler les bandes pour un affichage en
    vrais couleurs (rouge, vert, bleu).

    Ouvrez les neuf images et remarquez les changements.

    Les fichiers ~training/training.shp~ et
    ~testing/testing.shp~ contiennent des
    polygones qui définissent 11 classes sur l'ensemble de la scène:

    |------+-----------------------------+------------|
    | Code | Nom                         | #polygones |
    |------+-----------------------------+------------|
    |   11 | Été                         |       7898 |
    |   12 | Hiver                       |       8171 |
    |   31 | Foret feuilles caduques     |        867 |
    |   32 | Foret feuilles persistantes |        125 |
    |   34 | Pelouses                    |         45 |
    |   36 | Lande ligneuse              |        386 |
    |   41 | Bati                        |       4719 |
    |   51 | Eau                         |       1280 |
    |  211 | Prairie                     |       5647 |
    |  221 | Verger                      |        204 |
    |  222 | Vigne                       |        559 |
    |------+-----------------------------+------------|


    Ouvrez un des fichiers de polygones dans QGIS. La table d'attributs est
    accessible depuis clic-droit sur la couche -> /Ouvrir la table des attributs/.
    Chaque label est visible et la liste est filtrable par expression SQL.

    Les polygones sont répartis en deux ensembles: apprentissage (training) et
    validation (testing).

**** Préparation des données

     Pour ce TP nous allons utiliser trois types d'images:
     1. Une image contenant les bandes originales pour toutes les dates (63 bandes)
     2. Une image contenant les valeurs du NDVI pour chaque date (9 bandes)
     3. Un image contenant les bandes originales et les valeurs du NDVI (72 bandes)


     La première étape consiste à générer ces différentes images.


     Utiliser l'application *ConcatenateImages* pour générer l'image
     contenant toutes les bandes.


     Utiliser l'application *RadiometricIndices* pour calculer le NDVI
     pour chaque date. Utiliser ensuite l'application
     *ConcatenateImages* pour générer l'image contenant l'ensemble des
     valeurs du NDVI (profil).


     Enfin, utiliser à nouveau l'application *ConcatenateImages* pour
     générer l'image contenant l'ensemble des bandes originales et le
     profil de NDVI.

     *Notes :* Le calcul de l'indice NDVI pour chaque date peut se
     faire à l'aide d'un script bash:

     \begin{verbatim}
     $ for f in *.tif; do \
       otbcli_RadiometricIndices -channels.blue 1 -channels.green 2 \
                                 -channels.red 3 -channels.nir 4 \
                                 -in "$f" -out "${f%.*}_ndvi.tif" \
                                 -list Vegetation:NDVI; done
     \end{verbatim}

**** Calcul des statistiques des échantillons disponibles

     Nous allons maintenant utiliser l'application
     *PolygonClassStatistics* afin de recenser les échantillons
     disponibles pour chaque classe et chaque polygone des données
     d'entraînement et de validation.

     Cette application prend en paramètre l'image qui sera utilisée
     pour extraire les échantillons, la donnée vecteur définissant les
     polygones à analyser (~training.shp~ pour l'apprentissage,
     ~testing.shp~ pour la validation), et le nom du champ définissant
     la classe dans ces données vecteur. Elle produit en sortie un
     fichier XML résumant les informations récoltées.

     En inspectant les données dans QGis, repéré le champ qui sera
     utilisé pour définir la classe.

     Ouvrez les fichiers XML générés. Combien y-a-t-il d'échantillons
     disponibles pour la classe ~prairie~ dans le jeu d'apprentissage
     ? Et dans celui de validation ?

     Combien d'échantillons contient le polygone dont l'identifiant
     est 1081 dans le jeu d'apprentissage ?

     Combien y-a-t-il d'échantillons, toutes classes confondues, dans
     le jeu d'apprentissage ?

**** Sélection des positions des échantillons

     Nous avons vu lors de l'étape précédente que les polygones
     d'apprentissage contienne beaucoup plus d'échantillons que
     nécessaire à l'apprentissage du modèle. Nous allons maintenant
     sélectionner un sous-ensemble de ces échantillons pour entraîner
     notre modèle à l'aide de l'application *SampleSelection*.

     Inspecter la documentation de l'application. Quels sont les
     différentes stratégies possibles pour réaliser cette sélection ?

     Nous allons utiliser la stratégie ~smallest~, qui permet de
     limiter le nombre d'échantillon par classe à celui de la classe
     la moins représentée. Nous génèrerons un jeu d'échantillons pour
     l'apprentissage (à partir de ~training.shp~) et un pour la
     validation (à partir de ~testing.shp~).

     L'application accepte comme paramètre l'image, la donnée vecteur
     contenant les polygones, le nom du champ correspondant à la
     classe dans cette donnée et le fichier de statistiques produit
     précédemment. Elle génère un fichier vecteur contenant des
     points, chaque point correspondant à un échantillon sélectionné.

     Combien d'échantillons au total ont été sélectionnés par classe
     pour l'apprentissage ?

     Ouvrir les fichiers de points ainsi généré dans QGis. Quels sont les
     attributs associés à chaque point ?
     
**** Calcul des attributs des échantillons

     Maintenant que nous avons sélectionné les positions des
     échantillons, nous allons leurs associer des attributs qui seront
     utilisés pour l'apprentissage et la classification. Ce calcul des
     attributs se fait à l'aide de l'application *SampleExtraction*,
     qui à chaque point va associer la signature spectrale de l'image
     sous-jacente.

     Notez qu'en l'absence d'une sortie vecteur explicite,
     l'application fonctionne en mode mise-à-jour de la donnée vecteur
     d'entrée, c'est à dire des attributs lui seront ajoutés.
     
     L'application prend en paramètre l'image à utiliser ainsi que le
     fichier de points à enrichir. Elle permet également de spécifier
     le nom des champs qui seront générés. Ainsi si l'on spécifie
     comme préfixe ~band_~, alors les champs s'appelleront ~band_0~,
     ~band_1~, etc ...

     Nommer les bandes de l'image avec le prefixe ~band_~ et le profil
     de NDVI avec le préfixe ~ndvi_~

     Utilisez l'application pour ajouter les attributs correspondant
     aux bandes de l'image et au profil de NDVI, pour le jeu
     d'apprentissage et le jeu de validation.

**** Entraînement du modèle

     Nos échantillons sont prêts, il est à présent temps d'entraîner
     notre modèle. Pour cela nous allons utiliser l'application
     *TrainVectorClassifier*, qui va lire les fichiers de points avec
     leurs attributs générés précédemment, et les utiliser pour
     réaliser l'apprentissage et la validation.

     Pour l'ensemble des expérience de cette section, nous allons
     utiliser un modèle de type *Random Forest* (rf), avec une
     profondeur maximale d'arbre de 20.

     Il faut à nouveau donner en paramètre à l'application le nom du
     champ définissant la classe. L'application accepte également en
     paramètre le fichier de point décrivant les échantillons et un
     fichier de sortie correspondant au modèle appris.

     Réaliser un premier apprentissage en utilisant uniquement la
     première date (~band_0~ à ~band_6~), et sans spécifier de fichier
     de validation.

     Quelle performance (coefficient Kappa) obtenez vous ? Cette
     performance vous parait elle réaliste ? Comment a-t-elle été
     évaluée ?

     Réaliser le même apprentissage, cette fois ci en ajoutant un fichier
     de validation que vous avez calculé à l'étape
     précédente. Qu'observez vous ?

     A l'aide de l'application d'apprentissage, il est aisé de tester
     des combinaisons d'attributs pour observer leur influence sur les
     performances:
     - Quelle date fourni la meilleure performance individuelle ?
     - La bande bleu côtier a-t-elle un intérêt pour la classification
       ?
     - Quelle est la performance du profil de ndvi seul ?
     - Quelle est la performance en utilisant toutes les bandes sans
       le profil de NDVI ?
     - Quelle est la performance en utilisant toutes les bandes et le
       profil de NDVI ?

     *Notes :* Il est possible de définir des variables bash pour les
     ensembles d'attributs:

     \begin{verbatim}
     date1=`for i in $(seq 0 6); do printf "band_$i "; done`
     date2=`for i in $(seq 7 13); do printf "band_$i "; done`
     date3=`for i in $(seq 14 20); do printf "band_$i "; done`
     date4=`for i in $(seq 21 27); do printf "band_$i "; done`
     date5=`for i in $(seq 28 34); do printf "band_$i "; done`
     date6=`for i in $(seq 35 41); do printf "band_$i "; done`
     date7=`for i in $(seq 42 48); do printf "band_$i "; done`
     date8=`for i in $(seq 49 55); do printf "band_$i "; done`
     date9=`for i in $(seq 56 62); do printf "band_$i "; done`

     date1_nocb=`for i in $(seq 1 6); do printf "band_$i "; done`

     bands=`for i in $(seq 0 62); do printf "band_$i "; done`

     ndvis=`for i in $(seq 0 8); do printf "ndvi_$i "; done`

     \end{verbatim}

     Ensuite, on peut directement utiliser ces variables en paramètres
     de l'application ~-feat ${bands} ${ndvis}~.

**** Classification

**** Post-traitements de la classification

  L'application /ClassificationMapRegularization/ filtre une image classifiée
  en utilisant un vote majoritaire local.

  Les paramètres à régler sont:

  - ip.radius 1 :: Rayon de la zone participant au vote
  - ip.suvbool 0 :: Que faire lors d'une égalité. 0 pour utiliser la valeur existante.

  Filtrez le résultat de la classification précédente (neuf dates et
  profil NDVI) puis comparer les résultats avec les images RGB (dans
  monteverdi) et avec les matrices de confusions (avec un tableur).



** Classification supervisée pour les séries multi-temporelles    :solutions:

*Note: * Dans cette solution, la variable d'environnement ~${LS8DATA}~
contient le chemin vers le répertoire /classification/ des données
fournies avec le TP.

*** Classification d'une date

    Premièrement, on calcule les statistiques de l'image à classer:
    
    #+BEGIN_EXAMPLE
    $ otbcli_ComputeImagesStatistics \
    -il ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20141026.tif \
                                     -out images_statistics.xml
    #+END_EXAMPLE

    On réalise ensuite l'apprentissage du modèle SVM.

    #+BEGIN_EXAMPLE
    $ otbcli_TrainImagesClassifier \
    -io.il ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20141026.tif \
      -io.vd ${LS8DATA}/training/training.shp \
      -sample.vfn CODE \
      -sample.vtr 0 \
      -classifier libsvm \
      -io.imstat images_statistics.xml \
      -io.out model.svm
    #+END_EXAMPLE

    Une fois le modèle appris, on l'utilise pour réaliser la classification:

    #+BEGIN_EXAMPLE
    $ otbcli_ImageClassifier \
    -in ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20141026.tif \
    -imstat images_statistics.xml \
    -model  model.svm \
    -out    labeled_image.tif
    #+END_EXAMPLE

    On peut ensuite estimer les performances obtenues à l'aide du jeu de validation:

    #+BEGIN_EXAMPLE
    $ otbcli_ComputeConfusionMatrix \
      -in labeled_image.tif \
      -ref vector \
      -ref.vector.in ${LS8DATA}/testing/testing.shp \
      -ref.vector.field CODE \
      -out confusion_matrix.csv
    #+END_EXAMPLE

    Pour une meilleure visualisation des résultats, on réalise une
    carte couleur en fonction de la classe attribuée:

    #+BEGIN_EXAMPLE
    $ otbcli_ColorMapping \
    -in labeled_image.tif \
    -method custom \
    -method.custom.lut ../../color_map.txt \
    -out RGB_color_image.tif
   #+END_EXAMPLE

    Les performances de cette premières classification ne sont pas
    très bonne: on va maintenant essayer d'améliorer la discrimination
    des classes de végétation en utilisant plusieurs dates.

*** Classification multi-date

    Pour commencer, on va concaténer les images de l'ensemble des
    dates en une seule image de $7x9=63$ bandes:

    #+BEGIN_EXAMPLE
    $ otbcli_ConcatenateImages -il \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140309.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140401.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140417.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140528.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140620.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140731.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140901.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20141003.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20141026.tif \
    -out image_concat.tif int16
    #+END_EXAMPLE
    
    Ensuite, on déroule le processus déjà réalisé lors de l'étape précédente.


    Calcul des statistiques:

    #+BEGIN_EXAMPLE
    $ otbcli_ComputeImagesStatistics -il \
    image_concat.tif \
    -out images_statistics.xml
    #+END_EXAMPLE


    Apprentissage du modèle (cette fois-ci, on utilise le classifieur random forests, avec plus d'échantillons):

    #+BEGIN_EXAMPLE
    $ otbcli_TrainImagesClassifier -io.il \
    image_concat.tif \
    -io.vd \
    ${LS8DATA}/training/training.shp \
    -sample.vfn CODE \
    -sample.vtr 0 \
    -classifier rf \
    -sample.bm 0 \
    -sample.mt 2000 \
    -classifier.rf.max 25 \
    -classifier.rf.min 25 \
    -io.imstat images_statistics.xml \
    -io.out model.rf
    #+END_EXAMPLE


    On réalise la carte de classification:

    #+BEGIN_EXAMPLE
    $ otbcli_ImageClassifier -in \
    image_concat.tif \
    -imstat images_statistics.xml \
    -model  model.rf \
    -out    labeled_image.tif
    #+END_EXAMPLE

    Et l'étape de validation:

     #+BEGIN_EXAMPLE
    $ otbcli_ComputeConfusionMatrix \
    -in labeled_image.tif \
    -ref vector \
    -ref.vector.in ${LS8DATA}/testing/testing.shp \
    -ref.vector.field CODE \
    -out confusion_matrix.csv
    #+END_EXAMPLE

    Puis la création de la carte colorisée:
 
    #+BEGIN_EXAMPLE
    $ otbcli_ColorMapping \
    -in labeled_image.tif \
    -method custom \
    -method.custom.lut ../../color_map.txt \
    -out RGB_color_image.tif
    #+END_EXAMPLE

    Dans cette seconde version, les performances sont nettement
    meilleures, mais il reste des confusions importantes, notamment
    pour les classes vergers et vignes.

*** Calcul d'un profil de NDVI

    Pour réaliser le profil de NDVI, on va utiliser l'application *RadiometricIndices* pour chacune des dates:

    #+BEGIN_EXAMPLE
    $ for date in "20140309" "20140401" "20140417" "20140528" \
    "20140620" "20140731" "20140901" "20141003" "20141026"; do \
    otbcli_RadiometricIndices \
    -in ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_${date}.tif \
    -out ${date}-ndvi.tif \
    -list Vegetation:NDVI \
    -channels.red 3 \
    -channels.nir 4 ; \
    done
    #+END_EXAMPLE
    
    Ensuite, on va concaténer l'ensemble des bandes NDVI en une seule image:

    #+BEGIN_EXAMPLE
    $ otbcli_ConcatenateImages -il \
    20140309-ndvi.tif \
    20140401-ndvi.tif \
    20140417-ndvi.tif \
    20140528-ndvi.tif \
    20140620-ndvi.tif \
    20140731-ndvi.tif \
    20140901-ndvi.tif \
    20141003-ndvi.tif \
    20141026-ndvi.tif \
    -out ndvi-profile.tif
    #+END_EXAMPLE

    Ce qui permet d'analyser le profil dans QGIS par exemple.

*** Classification du profil de NDVI
    
    Pour la classification, on rajoute également les bandes spectrales initiales pour toutes les dates:

    #+BEGIN_EXAMPLE
    $ otbcli_ConcatenateImages -il \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140309.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140401.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140417.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140528.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140620.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140731.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20140901.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20141003.tif \
    ${LS8DATA}/Extract16bits/LANDSAT_MultiTempIm_clip_GapF_20141026.tif \
    20140309-ndvi.tif \
    20140401-ndvi.tif \
    20140417-ndvi.tif \
    20140528-ndvi.tif \
    20140620-ndvi.tif \
    20140731-ndvi.tif \
    20140901-ndvi.tif \
    20141003-ndvi.tif \
    20141026-ndvi.tif \
    -out image_concat.tif
    #+END_EXAMPLE

    Nous déroulons ensuite les étapes classiques.

    Calcul des statistiques:

    #+BEGIN_EXAMPLE
    $ otbcli_ComputeImagesStatistics -il \
    image_concat.tif \
    -out images_statistics.xml
    #+END_EXAMPLE

    Apprentissage du modèle:

    #+BEGIN_EXAMPLE
    $ otbcli_TrainImagesClassifier -io.il \
    image_concat.tif \
    -io.vd \
    ${LS8DATA}/training/training.shp \
    -sample.vfn CODE \
    -sample.vtr 0 \
    -classifier rf \
    -sample.bm 0 \
    -sample.mt 2000 \
    -classifier.rf.max 25 \
    -classifier.rf.min 25 \
    -io.imstat images_statistics.xml \
    -io.out model.rf
    #+END_EXAMPLE

    Utilisation du modèle (classification):

    #+BEGIN_EXAMPLE
    $ otbcli_ImageClassifier -in \
    image_concat.tif \
    -imstat images_statistics.xml \
    -model  model.rf \
    -out    labeled_image.tif
    #+END_EXAMPLE

    Validation:

    #+BEGIN_EXAMPLE
    $ otbcli_ComputeConfusionMatrix \
    -in labeled_image.tif \
    -ref vector \
    -ref.vector.in ${LS8DATA}/testing/testing.shp \
    -ref.vector.field CODE \
    -out confusion_matrix.csv
    #+END_EXAMPLE

    Visualisation:

    #+BEGIN_EXAMPLE
    $ otbcli_ColorMapping \
    -in labeled_image.tif \
    -method custom \
    -method.custom.lut ../../color_map.txt \
    -out RGB_color_image.tif
    #+END_EXAMPLE

*** Post-traitements de la classification

    Pour régulariser la classification, on utilise l'application suivante:

    #+BEGIN_EXAMPLE
    $ otbcli_ClassificationMapRegularization \
    -io.in labeled_image.tif \
    -io.out regularized_image.tif \
    -ip.radius 1 \
    -ip.suvbool 0
    #+END_EXAMPLE
    

    On peut ensuite rejouer les étapes de colorisation et de validation:

    #+BEGIN_EXAMPLE
    $ otbcli_ColorMapping \
    -in regularized_image.tif \
    -method custom \
    -method.custom.lut ../../color_map.txt \
    -out rgb_regularized.tif
    #+END_EXAMPLE

    #+BEGIN_EXAMPLE
    $ otbcli_ComputeConfusionMatrix \
    -in regularized_image.tif \
    -ref vector \
    -ref.vector.in ${LS8DATA}/testing/testing.shp \
    -ref.vector.field CODE \
    -out confusion_matrix_regularized.csv
    #+END_EXAMPLE
